{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Cleaned 10 chunks from data/scraped_data/brand.vanderbilt.edu/brand.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 10 chunks from data/scraped_data/giving.vanderbilt.edu/giving.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 13 chunks from data/scraped_data/gradschool.vanderbilt.edu/gradschool.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 13 chunks from data/scraped_data/hr.vanderbilt.edu/hr.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 15 chunks from data/scraped_data/medschool.vanderbilt.edu/medschool.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 10 chunks from data/scraped_data/peabody.vanderbilt.edu/peabody.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 17 chunks from data/scraped_data/business.vanderbilt.edu/business.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 33 chunks from data/scraped_data/news.vanderbilt.edu/news.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/studentorg.vanderbilt.edu/studentorg.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 20 chunks from data/scraped_data/law.vanderbilt.edu/law.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 1 chunks from data/scraped_data/it.vanderbilt.edu/it.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/emergency.vanderbilt.edu/emergency.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 18 chunks from data/scraped_data/engineering.vanderbilt.edu/engineering.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 5 chunks from data/scraped_data/research.vanderbilt.edu/research.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 17 chunks from data/scraped_data/divinity.vanderbilt.edu/divinity.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 9 chunks from data/scraped_data/vanderbilt.edu/vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 10 chunks from data/scraped_data/as.vanderbilt.edu/as.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/campusdining.vanderbilt.edu/campusdining.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/info.engineering.vanderbilt.edu/info.engineering.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 15 chunks from data/scraped_data/admissions.vanderbilt.edu/admissions.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/wp0.vanderbilt.edu/wp0.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 1 chunks from data/scraped_data/events.vanderbilt.edu/events.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 48 chunks from data/scraped_data/www.vanderbilt.edu/www.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 8 chunks from data/scraped_data/blair.vanderbilt.edu/blair.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 11 chunks from data/scraped_data/my.vanderbilt.edu/my.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/www4.vanderbilt.edu/www4.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 7 chunks from data/scraped_data/nursing.vanderbilt.edu/nursing.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 11 chunks from data/scraped_data/registrar.vanderbilt.edu/registrar.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 1 chunks from data/scraped_data/publicsafety.vanderbilt.edu/publicsafety.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 1 chunks from data/scraped_data/commonplace.vanderbilt.edu/commonplace.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 7 chunks from data/scraped_data/finance.vanderbilt.edu/finance.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 2 chunks from data/scraped_data/police.vanderbilt.edu/police.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 3 chunks from data/scraped_data/studenthandbook.vanderbilt.edu/studenthandbook.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 1 chunks from data/scraped_data/printingservices.vanderbilt.edu/printingservices.vanderbilt.edu_part1.txt.json\n",
      "ğŸ§¹ Cleaned 7 chunks from data/scraped_data/eecs.vanderbilt.edu/eecs.vanderbilt.edu_part1.txt.json\n",
      "âœ… Finished cleaning all JSON files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the root directory containing JSON files\n",
    "ROOT_DIR = \"data/scraped_data\"\n",
    "\n",
    "def clean_json_file(json_path):\n",
    "    \"\"\"Removes chunks that contain only '=' characters from the JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks = json.load(f)\n",
    "\n",
    "        # Filter out chunks where the \"text\" field contains only '=' (any number of them)\n",
    "        cleaned_chunks = [chunk for chunk in chunks if not re.fullmatch(r\"=+\", chunk.get(\"text\", \"\").strip())]\n",
    "\n",
    "        if len(cleaned_chunks) != len(chunks):\n",
    "            print(f\"ğŸ§¹ Cleaned {len(chunks) - len(cleaned_chunks)} chunks from {json_path}\")\n",
    "\n",
    "            # Save the cleaned data back to the same JSON file\n",
    "            with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cleaned_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"âš ï¸ Skipping {json_path}: Invalid JSON format.\")\n",
    "\n",
    "def clean_all_json_files(root_dir):\n",
    "    \"\"\"Loops through all subfolders in the root directory and cleans each JSON file.\"\"\"\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                json_path = os.path.join(subdir, file)\n",
    "                clean_json_file(json_path)\n",
    "\n",
    "# Run the cleaning process\n",
    "if __name__ == \"__main__\":\n",
    "    clean_all_json_files(ROOT_DIR)\n",
    "    print(\"âœ… Finished cleaning all JSON files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Connecting to ChromaDB...\n",
      "ğŸ“Œ Total documents in 'WebsiteData': 6086\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define ChromaDB directory and collection name\n",
    "CHROMA_DB_DIR = \"../chroma_db\"\n",
    "COLLECTION_NAME = \"WebsiteData\"\n",
    "\n",
    "# Initialize ChromaDB\n",
    "print(\"ğŸ” Connecting to ChromaDB...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = Chroma(collection_name=COLLECTION_NAME, embedding_function=embeddings, persist_directory=CHROMA_DB_DIR)\n",
    "\n",
    "# Count the number of documents\n",
    "doc_count = vector_store._collection.count()\n",
    "print(f\"ğŸ“Œ Total documents in '{COLLECTION_NAME}': {doc_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
